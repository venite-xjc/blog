<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Jetcat&#39;s Blog Site</title>
    <link>https://venite-xjc.github.io/xjcblog/</link>
    <description>Recent content on Jetcat&#39;s Blog Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Jan 2023 01:50:11 +0800</lastBuildDate><atom:link href="https://venite-xjc.github.io/xjcblog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hi!</title>
      <link>https://venite-xjc.github.io/xjcblog/posts/first-post/</link>
      <pubDate>Thu, 26 Jan 2023 01:50:11 +0800</pubDate>
      
      <guid>https://venite-xjc.github.io/xjcblog/posts/first-post/</guid>
      <description>欢迎来到我的博客 我是一名大三本科生，专业是计算机科学与技术，同时对人工智能领域很有兴趣。
我的学习方向包括：Transformer, NeRF, GAN, novel view synthesis, GSN。</description>
    </item>
    
    <item>
      <title>attention in CV 学习笔记：Swin Transformer及部分代码复现</title>
      <link>https://venite-xjc.github.io/xjcblog/posts/md4_swintransformer/</link>
      <pubDate>Sat, 26 Nov 2022 01:50:11 +0800</pubDate>
      
      <guid>https://venite-xjc.github.io/xjcblog/posts/md4_swintransformer/</guid>
      <description>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 论文地址: https://arxiv.org/abs/2103.14030 参考代码：https://github.com/microsoft/Swin-Transformer 2021 ICCV best paper，提出了一个Transformer的通用backbone
远看是Transformer, 近看是CNN。
Transformer in CV的问题 ViT出来之后，虽然follow它的工作很多，但是很少有人能够提出一个通用的骨干网络来解决CV的各种任务。作者认为Transformer转换到CV领域的主要困难来自NLP和CV两大领域的形态不同，比如：
NLP中的问题规模和CV不一样，NLP的scale通常是固定的，但是CV中的scale是变化的。 NLP里面的token就是一个单词，整个序列往往不会太大，但是CV的分辨率是非常大的，计算复杂度以二次方增加。 Swin Transformer解决的方法：
在局部计算self-attention。 采用层级结构。 采用shift-window打通不同的window之间的信息。(模型的名字就来自shift-window) 左图是Swin Transformer的结构，把图像划分为patch，对在同一window之类的patch计算attention，然后通过合并patch降低分辨率，不断重复操作。右图是ViT的结构，把图像划分为patch，然后始终在所有patch之间做self-attention。
对比两个结构，Swin Transformer基本放弃使用了ViT的思想，转而借鉴了很多CNN的结构。比如说局部self-attention对比局部卷积域，shift-window对比滑动窗口，并且两者都采用了层叠结构。ViT讲求的是始终考虑全局所有信息计算attenntion，而Swin Transformer仅考虑窗口内的信息计算attention。两者都有降低计算复杂度的目的，但是一个希望保持注意力计算域在整张图片上，所以着力减少token数量，另一个选择了减小注意力计算域，然后通过层叠结构逐步扩展到整张图片上，所以可以把patch划得更小。
Swin主要结构 参照上图，因为Transformer操作的基本单位都是patch，所以Swin Transformer也是首先进行patch partition，文中采用的是4×4的大小。这样一个patch就会包含4×4×3=48的维度，经过linear embedding layer把维度投影到C，分辨率变为$\frac{H}{4}\times\frac{W}{4}$。然后会经过两个Swin Transformer Block计算注意力。这个过程属于上图的stage1。
在stage2、stage3、stage4中，首先会把相邻的2×2个patch 融合到一起成为新patch，这样得到的patch维度会是4C，需要使用一个linear layer把维度投影到2C。之后经过数量不等的Swin Transformer Block计算注意力。经过这样的一个流程，特征图的分辨率会从$\frac{H}{4}\times\frac{W}{4}$依次下降到$\frac{H}{8}\times\frac{W}{8}$、$\frac{H}{16}\times\frac{W}{16}$、$\frac{H}{32}\times\frac{W}{32}$，也就是一个分层结构。
Swin Transformer Block结构 Swin Transformer Block是Swin Transformer的关键。标准的Transformer都是在全局进行注意力计算的，但是这样会导致计算复杂度二次上升。Swin Transformer采用了在window中进行计算的方式。
A×B大小的矩阵和B×C大小的矩阵相乘计算复杂度为ABC，在self-attention中，所有的线性变换都是以矩阵乘法的形式得到的。self-attention的公式如下：
$$\text{Attention}(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
对于分辨率(patch数量)为$h \times w$的特征图，它的输入为$hw\times C$，首先需要经过三个$C\times C$的矩阵分别得到Q，K，V，那么复杂度一共就为$3hwC^2$。 计算$QK^T$的时候，是$hw\times C$的矩阵和$C\times hw$的矩阵相乘，计算复杂度为$(hw)^2C$。 计算$(QK^T)V$的时候，是$hw\times hw$的矩阵和$hw\times C$的矩阵相乘，计算复杂度为$(hw)^2C$。 在输出的时候还需要经过一次线性变换，乘上一个$C\times C$的矩阵，计算复杂度为$hwC^2$。</description>
    </item>
    
    <item>
      <title>attention in CV 学习笔记：ViT</title>
      <link>https://venite-xjc.github.io/xjcblog/posts/md3_vit/</link>
      <pubDate>Sat, 26 Nov 2022 01:50:11 +0800</pubDate>
      
      <guid>https://venite-xjc.github.io/xjcblog/posts/md3_vit/</guid>
      <description>AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE 论文地址：https://arxiv.org/abs/2010.11929 Transformer in CV 在这篇文章之前，已经有很多工作是关于如何把attention应用到CV任务上，有的是把attention添加到模型里面，要么是替换掉一些CNN的组件。但是很少有任务是把纯attention网络应用到CV上的。
而在NLP领域，Transformer已经成为了主流方法，由于Transformer的long-range和缺乏归纳偏置的特性，Transformer通常是需要在超大规模数据集上进行预训练然后在小规模数据集上fine-tune。同时，Transformer至今没有出现性能饱和的现象，这就让我们在超大规模数据集上训练一个超大模型成为可能。
鉴于CV领域并没有有效地推广Transformer，作者希望能够在尽量少的改变标准的Transformer的情况下把Transformer应用到CV领域。
这篇文章在Transformer in CV领域地位非常高，可以说直接导致了Transformer在CV的大火，在这篇文章出来的几个月之内，就有很多篇文章follow他们的工作到其他方面。它的主要贡献我概括为以下几点：
证明了nlp里面的Transformer架构是可以直接迁移到CV的。 提出了如何适配Transformer处理图像的问题。 分析了Transformer和CNN的区别，并且提出了如何训练ViT（a lot of money, a lot of GPUs）。 再次证明了google的算力不是开玩笑的。 An image is worth 16×16 words 这篇文章的心心念念地就是把标准Transformer迁移到CV领域，所以本篇文章的Transformer和nlp里面的Transformer保持了一致。
一个问题是，在attention is all you need里面，我们知道了Transformer的输入是一个序列，如何把图像处理成一个序列呢？一个非常简单的方法就是reshape一把梭哈，直接把[C,H,W]变成[C, H×W]就是一个典型的序列了，正如Non-local Net的做法。
然而，一般图像的大小是224×224或者256×256，如果按照pixel展开的话得有50176的长度或者65536的长度，让他们去做self-attention计算量空前的大。non-local对此给出的解决办法是尽量放在模型的后面，以及采用bottleneck来降低分辨率，GCNet采用的方法是直接回避掉相互计算attention部分，所以它可以放在任意一层。但是都不太适用于Transformer，毕竟ViT打出的旗帜就是“没有卷积”，这些操作都需要卷积参与。
Transformer针对这个问题采用的就是划分patch的方法，把patch替换pixel作为一个token。如果 $(P, P)$ 是一个patch的分辨率的话，对于一个 $x\in \mathbb{R}^{H×W×C}$ 就可以划分成 $x_p\in \mathbb{R}^{N×(P^2\cdot C)}$ 的序列，其中 $N=HW/P^2$ 。假设patch边长是16的话，一个256×256的图片就是有16×16个patch，因为nlp里面的token对应的是一个单词，所以就有了本文标题：一张图片值16×16个单词。
这个划分patch来降低token个数的方法基本被后面的Transformer继承。
如何适配Transformer 我们已经知道了ViT是如何构造输入token，一个token的维度为$p^2\cdot C$，所以和标准Transformer一样，需要首先进行一个线性映射来把维度映射到Transformer的维度D。
文章还沿用了BERT的方法，加入了一个class token,并且把这个token定义为了$z^0_0=X_{class}$，相当于图像在(0,0)位置增加了一个token。在训练过程中，这个token和其他token是等价的，所以会逐渐融合到其他token的信息，在最后做分类人物的时候就不需要提取所有的输出了，直接把这个token的信息提取出来接上一个分类头就可以了。
在预训练的时候分类头是一个MLP，在微调的时候分类头是一个单层线性层。
对position encoding，作者也做了实验，发现一个1D的可学习的position embedding并不比2D的position embedding表现差，所以文章后面的实验就直接使用了这个1D的position embedding。</description>
    </item>
    
    <item>
      <title>attention in CV学习笔记：Attention is All you Need</title>
      <link>https://venite-xjc.github.io/xjcblog/posts/md1_attention_is_all_you_need/</link>
      <pubDate>Sat, 26 Nov 2022 01:50:11 +0800</pubDate>
      
      <guid>https://venite-xjc.github.io/xjcblog/posts/md1_attention_is_all_you_need/</guid>
      <description>从NLP到CV，attention机制以及Transformer模型近几年大放异彩，大有一统江湖之势。Transformer作为一个long-range模型，一方面相对于CNN在很多需要全局信息的任务上更加有优势，另一方面具有统一NLP和CV的潜力，所以被人广泛研究（灌水）。作为一个深度学习方向的的实习生，在过去的一年零零散散看了不少attention的paper，但是始终对这个模型没有形成体系的认知，学习也颇为囫囵吞枣。所以希望重新整理一下自己看过的paper，同时记录一下自己的笔记。
Attention is All you Need 论文地址：https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf 参考代码：https://github.com/jadore801120/attention-is-all-you-need-pytorch 这篇首先提出了Transformer这个完全依赖注意力机制的模型结构。
啥叫self-attention? Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.
self-attention是一种把单个序列的不同位置关联起来来计算序列表示的一种注意力机制
这句话已经说得很明确了，其一说明了self-attention这个self代表序列内部的计算，其二说明了self-attention会考虑到序列的所有位置，也就是global information。
啥叫attention? An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</description>
    </item>
    
    <item>
      <title>attention in CV学习笔记：Non-local Net 与 GCNet</title>
      <link>https://venite-xjc.github.io/xjcblog/posts/md2_non_local_gcnet/</link>
      <pubDate>Sat, 26 Nov 2022 01:50:11 +0800</pubDate>
      
      <guid>https://venite-xjc.github.io/xjcblog/posts/md2_non_local_gcnet/</guid>
      <description>Non-local Neural Networks 论文地址：https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html 感觉这篇算视觉领域引入attention的先例了，主要的工作就是介绍了什么是non-local，non-local的技术细节，以及non-local在具体任务中的实现。
什么是non-local? 这篇文章是受到non-local mean算法的启发，定义了一个non-local operation，原文中定义是这样的：
In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions.
在这篇文章中，我们将非局部操作作为一个用于捕获长程依赖关系的通用模块家族。受到计算机视觉中非局部均值的经典算法的启发，我们的非局部操作将一个位置（在操作中的）的响应计算为所有位置的加权和。
看到long-range和weighted sum of the features at all positions，相信大家也都明白了，这不就是attention嘛，只不过这一篇的思想来源是non-local means算法。不过attention跟non-local means这两个算法，你说它们是一个东西，它们的思想又不是完全一样的；你说他们不是一个东西吧，这两个的形式又基本一致……
non-local means算法推导 non-local means是一个用于降噪的算法。我们通常的图像去噪方法就是用一个像素附近像素的均值来代替它。对于一个像素来说，如果我们在一张图片里面找到与这个像素最相似的九个像素，我们就可以把这个噪声降低三倍。但是最相近的像素不一定隔得很近，所以这篇文章的方法就是通过扫描窗口得到最相似的像素，然后用它们的均值来进行去噪。这个新的filter就是： $$ NLu(o)=\frac{1}{C(p)}\int f(d(B(p), B(q)))u(q)dq\\ $$ $d(B(p), B(q))$代表以$p, q$为中心的patch的欧氏距离,$f$是一个减函数，$C(p)$是一个归一化因子,$B$代表一个patch。算出来的欧式距离越小，$u(q)$前面的系数越高。</description>
    </item>
    
  </channel>
</rss>
