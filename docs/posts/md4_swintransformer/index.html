<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>attention in CV 学习笔记：Swin Transformer及部分代码复现 | My Blog Site</title>
<meta name="keywords" content="">
<meta name="description" content="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 论文地址: https://arxiv.org/abs/2103.14030 参考代码：https://github.com/microsoft/Swin-Transformer 2021 ICCV best paper，提出了一个Transformer的通用backbone
远看是Transformer, 近看是CNN。
Transformer in CV的问题 ViT出来之后，虽然follow它的工作很多，但是很少有人能够提出一个通用的骨干网络来解决CV的各种任务。作者认为Transformer转换到CV领域的主要困难来自NLP和CV两大领域的形态不同，比如：
NLP中的问题规模和CV不一样，NLP的scale通常是固定的，但是CV中的scale是变化的。 NLP里面的token就是一个单词，整个序列往往不会太大，但是CV的分辨率是非常大的，计算复杂度以二次方增加。 Swin Transformer解决的方法：
在局部计算self-attention。 采用层级结构。 采用shift-window打通不同的window之间的信息。(模型的名字就来自shift-window) 左图是Swin Transformer的结构，把图像划分为patch，对在同一window之类的patch计算attention，然后通过合并patch降低分辨率，不断重复操作。右图是ViT的结构，把图像划分为patch，然后始终在所有patch之间做self-attention。
对比两个结构，Swin Transformer基本放弃使用了ViT的思想，转而借鉴了很多CNN的结构。比如说局部self-attention对比局部卷积域，shift-window对比滑动窗口，并且两者都采用了层叠结构。ViT讲求的是始终考虑全局所有信息计算attenntion，而Swin Transformer仅考虑窗口内的信息计算attention。两者都有降低计算复杂度的目的，但是一个希望保持注意力计算域在整张图片上，所以着力减少token数量，另一个选择了减小注意力计算域，然后通过层叠结构逐步扩展到整张图片上，所以可以把patch划得更小。
Swin主要结构 参照上图，因为Transformer操作的基本单位都是patch，所以Swin Transformer也是首先进行patch partition，文中采用的是4×4的大小。这样一个patch就会包含4×4×3=48的维度，经过linear embedding layer把维度投影到C，分辨率变为$\frac{H}{4}\times\frac{W}{4}$。然后会经过两个Swin Transformer Block计算注意力。这个过程属于上图的stage1。
在stage2、stage3、stage4中，首先会把相邻的2×2个patch 融合到一起成为新patch，这样得到的patch维度会是4C，需要使用一个linear layer把维度投影到2C。之后经过数量不等的Swin Transformer Block计算注意力。经过这样的一个流程，特征图的分辨率会从$\frac{H}{4}\times\frac{W}{4}$依次下降到$\frac{H}{8}\times\frac{W}{8}$、$\frac{H}{16}\times\frac{W}{16}$、$\frac{H}{32}\times\frac{W}{32}$，也就是一个分层结构。
Swin Transformer Block结构 Swin Transformer Block是Swin Transformer的关键。标准的Transformer都是在全局进行注意力计算的，但是这样会导致计算复杂度二次上升。Swin Transformer采用了在window中进行计算的方式。
A×B大小的矩阵和B×C大小的矩阵相乘计算复杂度为ABC，在self-attention中，所有的线性变换都是以矩阵乘法的形式得到的。self-attention的公式如下：
$$\text{Attention}(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
对于分辨率(patch数量)为$h \times w$的特征图，它的输入为$hw\times C$，首先需要经过三个$C\times C$的矩阵分别得到Q，K，V，那么复杂度一共就为$3hwC^2$。 计算$QK^T$的时候，是$hw\times C$的矩阵和$C\times hw$的矩阵相乘，计算复杂度为$(hw)^2C$。 计算$(QK^T)V$的时候，是$hw\times hw$的矩阵和$hw\times C$的矩阵相乘，计算复杂度为$(hw)^2C$。 在输出的时候还需要经过一次线性变换，乘上一个$C\times C$的矩阵，计算复杂度为$hwC^2$。">
<meta name="author" content="">
<link rel="canonical" href="https://venite-xjc.github.io/posts/md4_swintransformer/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://venite-xjc.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://venite-xjc.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://venite-xjc.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://venite-xjc.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://venite-xjc.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>



<meta property="og:title" content="attention in CV 学习笔记：Swin Transformer及部分代码复现" />
<meta property="og:description" content="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 论文地址: https://arxiv.org/abs/2103.14030 参考代码：https://github.com/microsoft/Swin-Transformer 2021 ICCV best paper，提出了一个Transformer的通用backbone
远看是Transformer, 近看是CNN。
Transformer in CV的问题 ViT出来之后，虽然follow它的工作很多，但是很少有人能够提出一个通用的骨干网络来解决CV的各种任务。作者认为Transformer转换到CV领域的主要困难来自NLP和CV两大领域的形态不同，比如：
NLP中的问题规模和CV不一样，NLP的scale通常是固定的，但是CV中的scale是变化的。 NLP里面的token就是一个单词，整个序列往往不会太大，但是CV的分辨率是非常大的，计算复杂度以二次方增加。 Swin Transformer解决的方法：
在局部计算self-attention。 采用层级结构。 采用shift-window打通不同的window之间的信息。(模型的名字就来自shift-window) 左图是Swin Transformer的结构，把图像划分为patch，对在同一window之类的patch计算attention，然后通过合并patch降低分辨率，不断重复操作。右图是ViT的结构，把图像划分为patch，然后始终在所有patch之间做self-attention。
对比两个结构，Swin Transformer基本放弃使用了ViT的思想，转而借鉴了很多CNN的结构。比如说局部self-attention对比局部卷积域，shift-window对比滑动窗口，并且两者都采用了层叠结构。ViT讲求的是始终考虑全局所有信息计算attenntion，而Swin Transformer仅考虑窗口内的信息计算attention。两者都有降低计算复杂度的目的，但是一个希望保持注意力计算域在整张图片上，所以着力减少token数量，另一个选择了减小注意力计算域，然后通过层叠结构逐步扩展到整张图片上，所以可以把patch划得更小。
Swin主要结构 参照上图，因为Transformer操作的基本单位都是patch，所以Swin Transformer也是首先进行patch partition，文中采用的是4×4的大小。这样一个patch就会包含4×4×3=48的维度，经过linear embedding layer把维度投影到C，分辨率变为$\frac{H}{4}\times\frac{W}{4}$。然后会经过两个Swin Transformer Block计算注意力。这个过程属于上图的stage1。
在stage2、stage3、stage4中，首先会把相邻的2×2个patch 融合到一起成为新patch，这样得到的patch维度会是4C，需要使用一个linear layer把维度投影到2C。之后经过数量不等的Swin Transformer Block计算注意力。经过这样的一个流程，特征图的分辨率会从$\frac{H}{4}\times\frac{W}{4}$依次下降到$\frac{H}{8}\times\frac{W}{8}$、$\frac{H}{16}\times\frac{W}{16}$、$\frac{H}{32}\times\frac{W}{32}$，也就是一个分层结构。
Swin Transformer Block结构 Swin Transformer Block是Swin Transformer的关键。标准的Transformer都是在全局进行注意力计算的，但是这样会导致计算复杂度二次上升。Swin Transformer采用了在window中进行计算的方式。
A×B大小的矩阵和B×C大小的矩阵相乘计算复杂度为ABC，在self-attention中，所有的线性变换都是以矩阵乘法的形式得到的。self-attention的公式如下：
$$\text{Attention}(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
对于分辨率(patch数量)为$h \times w$的特征图，它的输入为$hw\times C$，首先需要经过三个$C\times C$的矩阵分别得到Q，K，V，那么复杂度一共就为$3hwC^2$。 计算$QK^T$的时候，是$hw\times C$的矩阵和$C\times hw$的矩阵相乘，计算复杂度为$(hw)^2C$。 计算$(QK^T)V$的时候，是$hw\times hw$的矩阵和$hw\times C$的矩阵相乘，计算复杂度为$(hw)^2C$。 在输出的时候还需要经过一次线性变换，乘上一个$C\times C$的矩阵，计算复杂度为$hwC^2$。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://venite-xjc.github.io/posts/md4_swintransformer/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-26T01:50:11+08:00" />
<meta property="article:modified_time" content="2022-11-26T01:50:11+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="attention in CV 学习笔记：Swin Transformer及部分代码复现"/>
<meta name="twitter:description" content="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 论文地址: https://arxiv.org/abs/2103.14030 参考代码：https://github.com/microsoft/Swin-Transformer 2021 ICCV best paper，提出了一个Transformer的通用backbone
远看是Transformer, 近看是CNN。
Transformer in CV的问题 ViT出来之后，虽然follow它的工作很多，但是很少有人能够提出一个通用的骨干网络来解决CV的各种任务。作者认为Transformer转换到CV领域的主要困难来自NLP和CV两大领域的形态不同，比如：
NLP中的问题规模和CV不一样，NLP的scale通常是固定的，但是CV中的scale是变化的。 NLP里面的token就是一个单词，整个序列往往不会太大，但是CV的分辨率是非常大的，计算复杂度以二次方增加。 Swin Transformer解决的方法：
在局部计算self-attention。 采用层级结构。 采用shift-window打通不同的window之间的信息。(模型的名字就来自shift-window) 左图是Swin Transformer的结构，把图像划分为patch，对在同一window之类的patch计算attention，然后通过合并patch降低分辨率，不断重复操作。右图是ViT的结构，把图像划分为patch，然后始终在所有patch之间做self-attention。
对比两个结构，Swin Transformer基本放弃使用了ViT的思想，转而借鉴了很多CNN的结构。比如说局部self-attention对比局部卷积域，shift-window对比滑动窗口，并且两者都采用了层叠结构。ViT讲求的是始终考虑全局所有信息计算attenntion，而Swin Transformer仅考虑窗口内的信息计算attention。两者都有降低计算复杂度的目的，但是一个希望保持注意力计算域在整张图片上，所以着力减少token数量，另一个选择了减小注意力计算域，然后通过层叠结构逐步扩展到整张图片上，所以可以把patch划得更小。
Swin主要结构 参照上图，因为Transformer操作的基本单位都是patch，所以Swin Transformer也是首先进行patch partition，文中采用的是4×4的大小。这样一个patch就会包含4×4×3=48的维度，经过linear embedding layer把维度投影到C，分辨率变为$\frac{H}{4}\times\frac{W}{4}$。然后会经过两个Swin Transformer Block计算注意力。这个过程属于上图的stage1。
在stage2、stage3、stage4中，首先会把相邻的2×2个patch 融合到一起成为新patch，这样得到的patch维度会是4C，需要使用一个linear layer把维度投影到2C。之后经过数量不等的Swin Transformer Block计算注意力。经过这样的一个流程，特征图的分辨率会从$\frac{H}{4}\times\frac{W}{4}$依次下降到$\frac{H}{8}\times\frac{W}{8}$、$\frac{H}{16}\times\frac{W}{16}$、$\frac{H}{32}\times\frac{W}{32}$，也就是一个分层结构。
Swin Transformer Block结构 Swin Transformer Block是Swin Transformer的关键。标准的Transformer都是在全局进行注意力计算的，但是这样会导致计算复杂度二次上升。Swin Transformer采用了在window中进行计算的方式。
A×B大小的矩阵和B×C大小的矩阵相乘计算复杂度为ABC，在self-attention中，所有的线性变换都是以矩阵乘法的形式得到的。self-attention的公式如下：
$$\text{Attention}(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
对于分辨率(patch数量)为$h \times w$的特征图，它的输入为$hw\times C$，首先需要经过三个$C\times C$的矩阵分别得到Q，K，V，那么复杂度一共就为$3hwC^2$。 计算$QK^T$的时候，是$hw\times C$的矩阵和$C\times hw$的矩阵相乘，计算复杂度为$(hw)^2C$。 计算$(QK^T)V$的时候，是$hw\times hw$的矩阵和$hw\times C$的矩阵相乘，计算复杂度为$(hw)^2C$。 在输出的时候还需要经过一次线性变换，乘上一个$C\times C$的矩阵，计算复杂度为$hwC^2$。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://venite-xjc.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "attention in CV 学习笔记：Swin Transformer及部分代码复现",
      "item": "https://venite-xjc.github.io/posts/md4_swintransformer/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "attention in CV 学习笔记：Swin Transformer及部分代码复现",
  "name": "attention in CV 学习笔记：Swin Transformer及部分代码复现",
  "description": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 论文地址: https://arxiv.org/abs/2103.14030 参考代码：https://github.com/microsoft/Swin-Transformer 2021 ICCV best paper，提出了一个Transformer的通用backbone\n远看是Transformer, 近看是CNN。\nTransformer in CV的问题 ViT出来之后，虽然follow它的工作很多，但是很少有人能够提出一个通用的骨干网络来解决CV的各种任务。作者认为Transformer转换到CV领域的主要困难来自NLP和CV两大领域的形态不同，比如：\nNLP中的问题规模和CV不一样，NLP的scale通常是固定的，但是CV中的scale是变化的。 NLP里面的token就是一个单词，整个序列往往不会太大，但是CV的分辨率是非常大的，计算复杂度以二次方增加。 Swin Transformer解决的方法：\n在局部计算self-attention。 采用层级结构。 采用shift-window打通不同的window之间的信息。(模型的名字就来自shift-window) 左图是Swin Transformer的结构，把图像划分为patch，对在同一window之类的patch计算attention，然后通过合并patch降低分辨率，不断重复操作。右图是ViT的结构，把图像划分为patch，然后始终在所有patch之间做self-attention。\n对比两个结构，Swin Transformer基本放弃使用了ViT的思想，转而借鉴了很多CNN的结构。比如说局部self-attention对比局部卷积域，shift-window对比滑动窗口，并且两者都采用了层叠结构。ViT讲求的是始终考虑全局所有信息计算attenntion，而Swin Transformer仅考虑窗口内的信息计算attention。两者都有降低计算复杂度的目的，但是一个希望保持注意力计算域在整张图片上，所以着力减少token数量，另一个选择了减小注意力计算域，然后通过层叠结构逐步扩展到整张图片上，所以可以把patch划得更小。\nSwin主要结构 参照上图，因为Transformer操作的基本单位都是patch，所以Swin Transformer也是首先进行patch partition，文中采用的是4×4的大小。这样一个patch就会包含4×4×3=48的维度，经过linear embedding layer把维度投影到C，分辨率变为$\\frac{H}{4}\\times\\frac{W}{4}$。然后会经过两个Swin Transformer Block计算注意力。这个过程属于上图的stage1。\n在stage2、stage3、stage4中，首先会把相邻的2×2个patch 融合到一起成为新patch，这样得到的patch维度会是4C，需要使用一个linear layer把维度投影到2C。之后经过数量不等的Swin Transformer Block计算注意力。经过这样的一个流程，特征图的分辨率会从$\\frac{H}{4}\\times\\frac{W}{4}$依次下降到$\\frac{H}{8}\\times\\frac{W}{8}$、$\\frac{H}{16}\\times\\frac{W}{16}$、$\\frac{H}{32}\\times\\frac{W}{32}$，也就是一个分层结构。\nSwin Transformer Block结构 Swin Transformer Block是Swin Transformer的关键。标准的Transformer都是在全局进行注意力计算的，但是这样会导致计算复杂度二次上升。Swin Transformer采用了在window中进行计算的方式。\nA×B大小的矩阵和B×C大小的矩阵相乘计算复杂度为ABC，在self-attention中，所有的线性变换都是以矩阵乘法的形式得到的。self-attention的公式如下：\n$$\\text{Attention}(Q,K,V)=softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n对于分辨率(patch数量)为$h \\times w$的特征图，它的输入为$hw\\times C$，首先需要经过三个$C\\times C$的矩阵分别得到Q，K，V，那么复杂度一共就为$3hwC^2$。 计算$QK^T$的时候，是$hw\\times C$的矩阵和$C\\times hw$的矩阵相乘，计算复杂度为$(hw)^2C$。 计算$(QK^T)V$的时候，是$hw\\times hw$的矩阵和$hw\\times C$的矩阵相乘，计算复杂度为$(hw)^2C$。 在输出的时候还需要经过一次线性变换，乘上一个$C\\times C$的矩阵，计算复杂度为$hwC^2$。",
  "keywords": [
    
  ],
  "articleBody": " Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 论文地址: https://arxiv.org/abs/2103.14030 参考代码：https://github.com/microsoft/Swin-Transformer 2021 ICCV best paper，提出了一个Transformer的通用backbone\n远看是Transformer, 近看是CNN。\nTransformer in CV的问题 ViT出来之后，虽然follow它的工作很多，但是很少有人能够提出一个通用的骨干网络来解决CV的各种任务。作者认为Transformer转换到CV领域的主要困难来自NLP和CV两大领域的形态不同，比如：\nNLP中的问题规模和CV不一样，NLP的scale通常是固定的，但是CV中的scale是变化的。 NLP里面的token就是一个单词，整个序列往往不会太大，但是CV的分辨率是非常大的，计算复杂度以二次方增加。 Swin Transformer解决的方法：\n在局部计算self-attention。 采用层级结构。 采用shift-window打通不同的window之间的信息。(模型的名字就来自shift-window) 左图是Swin Transformer的结构，把图像划分为patch，对在同一window之类的patch计算attention，然后通过合并patch降低分辨率，不断重复操作。右图是ViT的结构，把图像划分为patch，然后始终在所有patch之间做self-attention。\n对比两个结构，Swin Transformer基本放弃使用了ViT的思想，转而借鉴了很多CNN的结构。比如说局部self-attention对比局部卷积域，shift-window对比滑动窗口，并且两者都采用了层叠结构。ViT讲求的是始终考虑全局所有信息计算attenntion，而Swin Transformer仅考虑窗口内的信息计算attention。两者都有降低计算复杂度的目的，但是一个希望保持注意力计算域在整张图片上，所以着力减少token数量，另一个选择了减小注意力计算域，然后通过层叠结构逐步扩展到整张图片上，所以可以把patch划得更小。\nSwin主要结构 参照上图，因为Transformer操作的基本单位都是patch，所以Swin Transformer也是首先进行patch partition，文中采用的是4×4的大小。这样一个patch就会包含4×4×3=48的维度，经过linear embedding layer把维度投影到C，分辨率变为$\\frac{H}{4}\\times\\frac{W}{4}$。然后会经过两个Swin Transformer Block计算注意力。这个过程属于上图的stage1。\n在stage2、stage3、stage4中，首先会把相邻的2×2个patch 融合到一起成为新patch，这样得到的patch维度会是4C，需要使用一个linear layer把维度投影到2C。之后经过数量不等的Swin Transformer Block计算注意力。经过这样的一个流程，特征图的分辨率会从$\\frac{H}{4}\\times\\frac{W}{4}$依次下降到$\\frac{H}{8}\\times\\frac{W}{8}$、$\\frac{H}{16}\\times\\frac{W}{16}$、$\\frac{H}{32}\\times\\frac{W}{32}$，也就是一个分层结构。\nSwin Transformer Block结构 Swin Transformer Block是Swin Transformer的关键。标准的Transformer都是在全局进行注意力计算的，但是这样会导致计算复杂度二次上升。Swin Transformer采用了在window中进行计算的方式。\nA×B大小的矩阵和B×C大小的矩阵相乘计算复杂度为ABC，在self-attention中，所有的线性变换都是以矩阵乘法的形式得到的。self-attention的公式如下：\n$$\\text{Attention}(Q,K,V)=softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n对于分辨率(patch数量)为$h \\times w$的特征图，它的输入为$hw\\times C$，首先需要经过三个$C\\times C$的矩阵分别得到Q，K，V，那么复杂度一共就为$3hwC^2$。 计算$QK^T$的时候，是$hw\\times C$的矩阵和$C\\times hw$的矩阵相乘，计算复杂度为$(hw)^2C$。 计算$(QK^T)V$的时候，是$hw\\times hw$的矩阵和$hw\\times C$的矩阵相乘，计算复杂度为$(hw)^2C$。 在输出的时候还需要经过一次线性变换，乘上一个$C\\times C$的矩阵，计算复杂度为$hwC^2$。\n所以self-attention的复杂度一共为:$4hwC^2+2(hw)^2C$。\nWindow based Self-Attention前面得到QKV的过程和最后的线性变换和self-attention保持一致，但是计算$QK^T$变成了$\\frac{h}{M}\\times\\frac{w}{M}$个$(M\\times M)^2C$，计算$(QK^T)V$变成了$\\frac{h}{M}\\times\\frac{w}{M}$个$(M\\times M)^2C$。一共是$2M^2hwC$。\n所以Window based Self-Attention复杂度一共为$4hwC^2+2M^2hwC$。\n由于划分window的方法虽然能够降低计算复杂度，但是相当于破坏了attention的全局性，计算局限于window中，为了打破不同window之间的壁垒，作者采用了shift-window的方法。具体来说，就是把window移动$(\\lfloor\\frac{M}{2}\\rfloor, \\lfloor\\frac{M}{2}\\rfloor)$之后重新划分。参见下图：\n但是这样会带来问题，第一是window的个数由原来的4个变成了9个，第二是window中的patch数量不固定，所以作者采用了efficient batch computation的方法，如下图:\n其实也就是通过类似circular padding的方式把不完整的几个window给补全。\n但是在比如右下的window中，一个window包含了4个不同位置的patch，计算他们之间的attention是没有意义的，所以需要给它们添加mask，保证来自A的patch只跟来自A的patch算attention,来自B的patch只跟来自B的patch算attention。\n另外，Swin Transformer里面还添加了relative position bias，具体添加位置参照文中的公式，就是那个B：\n$$ \\text{Attention}(Q,K,V)=\\text{SoftMax}\\left(QK^T/\\sqrt{d}+B\\right)V $$\n源码里面是构造一个可以学习的位置参数表，然后再建立一个相对位置索引表。对于两个patch，首先计算它们的相对位置索引，然后根据相对位置索引去找到对应的bias。\n在一个Swin Transformer Block中，会首先进行一次不移动窗口的MSA计算以及依次移动窗口的MSA计算，这样整个过程表述为：\n$$ \\hat{z}^l=W-MSA(LN(z^{l-1}))+z^{l-1},\\\\ z^l = MLP(LN(\\hat{z}^l))+\\hat{z}^l,\\\\ \\hat{z}^{l+1} = SW-MSA(LN(z^l))+z^l,\\\\ z^{l+1} = MLP(LN(\\hat{z}^{l+1}))+\\hat{z}^{l+1}, $$ 代码复现 划分window以及逆过程代码:\ndef partition(input, window_size):\r'''\rinput: [B, C, H, W]\rwindows_size: int\r把图片变成window\rreturn: [B, N, C, window_size, window_wize] N=H//window_size*W//window_size\r'''\rassert len(input.shape) == 4\rB, Channel, H, W = input.shape\rassert H % window_size == 0 and W % window_size == 0\r#不考虑B和C，由于reshape不会改变底层一维数组分布，所以需要把H改成[窗口数，窗口里面的H数]\r#W改成[窗口数，窗口里面的W数]才符合按行排布的一维数组情况,顺序不能反。\routput = input.reshape(B, Channel, H//window_size, window_size, W//window_size, window_size)\r#由于我们想要的输出最后两维应该是一个patch的二维形式，所以需要把维度变换过去\routput = output.permute(0, 2, 1, 4, 3, 5).reshape(B, Channel, -1, window_size, window_size).permute(0, 2, 1, 3, 4)\rreturn output\rdef reverse(input):\r'''\rinput: [B, N, C, window_size, window_size]\rpartition的逆变换\rreturn: [B, C, H, W]\r'''\rassert len(input.shape) == 5\rB, N, C, window_size, _ = input.shape\rx = input.permute(0, 2, 1, 3, 4)\rx = x.reshape(B, C, int(N**0.5), int(N**0.5), window_size, window_size)\rx = x.permute(0, 1, 2, 4, 3, 5)\rx = x.reshape(B, C, int(N**0.5)*window_size, int(N**0.5)*window_size)\rreturn x\rpatch embedding代码:\nclass Patch_Linear_Embedding(nn.Module):\rdef __inti__(self, patch_size, dim_in, dim_out, norm = None):\r'''\r根据patch_size划分并且投影到dim_out上，仍然是二维的\r'''\rsuper(Patch_Linear_Embedding, self).__init__()\rself.linear_embedding = nn.Conv2d(dim_in, dim_out, kernel_size=patch_size, stride=patch_size, padding=0)\rself.norm = norm\rdef forward(self, input):\r'''\rinput: [B, C, H, W]\r'''\rx = self.linear_embedding(x)#[B, C, H, W]-\u003e[B, dim_out, num_patch, num_patch]\rx = x.reshape(x.shape[0], x.shape[1], -1)\rx = x.permute(0, 2, 1)\rif self.norm is not None:\rx = self.norm(x)\rreturn x\rMLP代码：\nclass MLP(nn.Module):\rdef __init__(self, dim_in, dim_out, dim_hidden, active_layer = nn.GELU, dropout = 0):\rsuper(MLP, self).__init__()\rself.fc1 = nn.Linear(dim_in, dim_hidden)\rself.active_layer = active_layer()\rself.fc2 = nn.Linear(dim_hidden, dim_out)\rself.Dropout = nn.Dropout(dropout)\rdef forward(self, x):\rx = self.fc1(x)\rx = self.active_layer(x)\rx = self.Dropout(x)#这里面使用了两个Dropout\rx = self.fc2(x)\rx = self.Dropout(x)\rreturn x\r添加relative position bias：\nclass Relative_Postion_Bias(nn.Module):\r'''\r输入: [B*N*num_head, L, C]\r'''\rdef __init__(self, num_head, M):\rsuper(Relative_Postion_Bias, self).__init__()\rself.bias = nn.Parameter(torch.zeros((2*M-1, 2*M-1, num_head)))#构造一个bias表\rself.num_head = num_head\r#以下查询方式都是基于window中的patch是按照行优先顺序展开的\rcoords = torch.stack(torch.meshgrid([torch.arange(M), torch.arange(M)]))\rcoords = torch.flatten(coords, 1)#绝对坐标\rrelative_coords = coords[:, :, None]-coords[:, None, :]#相对坐标\rrelative_coords = relative_coords.permute(1, 2, 0)#[M**2, M**2, 2]\rrelative_coords += M-1#索引置为非负数\rself.relative_coords = relative_coords\rdef forward(self, input):\r'''\rinput: [B*N*num_head, N, N]\r'''\rrelative_position_bias = self.bias[self.relative_coords[:, :, 0], self.relative_coords[:, :, 1]]\rrelative_position_bias = relative_position_bias.permute(2, 0, 1)\rrelative_position_bias = relative_position_bias.unsqueeze(0).repeat(input.shape[0]//self.num_head)\rrelative_position_bias = relative_position_bias.reshape(input.shape[0], input.shape[1], input.shape[2])\rreturn input + relative_position_bias\rpatch merging代码：\nclass Patch_Merging(nn.Module):\rdef __init__(self, dim_in, dim_out, stride):\rsuper(Patch_Merging, self).__init__()\rself.fc = nn.Linear(dim_in*stride**2, dim_out)\rself.stride = stride\rdef forward(self, input):\rx = partition(input, self.stride)\rx.resahpe(x.shape[0], x.shape[1], -1)\rx = self.fc(x)\rx = x.reshape(x.shape[0], x.shape[1], -1, 1, 1)\rx = reverse(x)\rreturn x\rW-MSA及SW-MSA：\nclass Window_Multihead_Attention(nn.Module):\rdef __init__(self, dim, num_head, window_size):\rself.dim = dim\rself.num_head = num_head\r#Swin默认这里的线性变换不改变维度，所以我也直接用了3*dim\r#Swin后面做multi-head的时候是直接把3*dim拆成num_head个来用，我认为在这里变成3*dim*num_head然后拆成num_head个效果是一样的\rself.qkv = nn.Linear(dim, 3*dim)\rself.relative_position_bias = Relative_Postion_Bias(num_head, window_size)\rdef forward(self, input, mask = None):\r'''\rinput: [B*N, L, C], L是把window中的patch按行优先展开的数量\rmask: [B*N, L, L], L同上\r'''\r#[B*N, L, C]-\u003e[B*N, L, 3, num_head, C//num_head]\rqkv = self.qkv(input).\\\rreshape(input.shape[0], input.shape[1], 3, self.num_head, input.shape[2]//self.num_head)\rqkv = qkv.permute(0, 3, 2, 1, 4).reshape(input.shape[0]*self.num_head, input.shape[1], 3, -1)\rQ = qkv[:, :, 0, :]#[B*N*num_head, L, C//num_head]\rK = qkv[:, :, 1, :]\rV = qkv[:, :, 2, :]\rattention = torch.bmm(Q, K.permute(0, 2, 1))/(self.dim//self.num_head)**0.5#计算相似度\rattention = self.relative_position_bias(attention)#加上Bias\rif mask is not None:\rweights = torch.exp(attention)\rweights = weights*(mask.unsqueeze(1).repeat(1, self.num_head, 1, 1)).reshape(-1, mask.shape[1], mask.shape[2])\rweights = weights/torch.sum(weights, -1)\relse:\rweights =torch.softmax(weights, -1)\routput = torch.bmm(weights, V)\rreturn output+input\rSwinTransformerBlock：\nclass SwinTransformerBlock(nn.Moduel):\rdef __init__(self, dim, num_head, window_size, mlp_dim_hidden):\rsuper(SwinTransformerBlock, self).__init__()\rself.LN1 = nn.LayerNorm(dim)\rself.LN2 = nn.LayerNorm(dim)\rself.LN3 = nn.LayerNorm(dim)\rself.LN4 = nn.LayerNorm(dim)\rself.WMSA = Window_Multihead_Attention(dim, num_head, window_size)\rself.SWMSA = Window_Multihead_Attention(dim, num_head, window_size)\rself.MLP1 = MLP(dim, dim, mlp_dim_hidden)\rself.MLP2 = MLP(dim, dim, mlp_dim_hidden)\rself.window_size = window_size\rdef forward(self, input):\r'''\rinput: [B, C, H, W]\r'''\rB, C, H, W = input.shape\rL = self.window_size**2\rN = H*W/L\rx = partition(x, self.window_size)\rx = x.reshape(B*N, C, L).permute(0, 2, 1)#[B*N, L, C]\rx = self.LN1(input)\rx = self.WMSA(x)\rx = x+input\rx = self.LN2(x)\rx = self.MLP1(x)\rx = x+input#[B*N, L, C]\rx = x.reshape(B, N, L, C).permute(0, 1, 3, 2).reshape(B, N, C, self.window_size, -1)\rx = reverse(x)\rshift = self.window_size//2\rx = torch.cat([torch.cat([x[:, :, shift:, shift:], x[:, :, shift:, :shift]], dim = -1), torch.cat([x[:, :, :shift, shift:], x[:, :, :shift, :shift]], dim = -1)], dim = -2)\rmask = torch.ones(B, 1, H, W)\rmask[:, :, :, -shift:] = 2\rmask[:, :, -shift:, :] = 3\rmask[:, :, -shift:, -shift:] = 4\rorigin = mask#[B, 1, H, W]\rmask = partition(mask, self.window_size)\rmask = mask.reshape(B, mask.shape[1], -1)#[B, N, L]\rmask = mask.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, self.window_size, self.window_size)#[B, N, L, ws, ws]\rmask = reverse(mask)#[B, L, H, W]\rmask = torch.where(mask==origin.repeat(1, L, 1, 1), 1, 0)\rx = self.LN3(input)\rx = self.SWMSA(x, mask)\rx = x+input\rx = self.LN4(x)\rx = self.MLP2(x)\rx = x+input#[B*N, L, C]\rreturn x\r实验结论 在分类任务上，Swin表现大幅超过其他模型。在Image-21k预训练的结果也是更好。\n在目标检测任务上，效果也是非常好。\nswin实现了在这种dense prediction任务上采用Transformer的backbone，效果也是大幅超越其他模型。\n关于shift-window以及relative position bias的消融实验。有了这两个东西都可以涨点。\n值得一提的是，作者后面把Swin的架构用到MLP-Mixer上面，发现了效果也很好，说明Swin的结构是具有通用性的。但是这是不是也说明了Swin的结构中，Transformer的地位其实没有那么高呢，我个人感觉Swin其实融合了很多文章的trick，比如滑动窗口也是一些CNN在用的提速方法，所以Swin刚出来的时候其实有很多质疑的声音。但是不得不说Swin作为通用backbone的地位非常高，我拿这个backbone也在某个workshop上干到过第二（虽然后面掉了），Transformer的潜力还是很大的。\n",
  "wordCount" : "857",
  "inLanguage": "en",
  "datePublished": "2022-11-26T01:50:11+08:00",
  "dateModified": "2022-11-26T01:50:11+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://venite-xjc.github.io/posts/md4_swintransformer/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My Blog Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://venite-xjc.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://venite-xjc.github.io/" accesskey="h" title="My Blog Site (Alt + H)">My Blog Site</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      attention in CV 学习笔记：Swin Transformer及部分代码复现
    </h1>
    <div class="post-meta"><span title='2022-11-26 01:50:11 +0800 CST'>November 26, 2022</span>

</div>
  </header> 
  <div class="post-content"><hr>
<h1 id="swin-transformer-hierarchical-vision-transformer-using-shifted-windows">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows<a hidden class="anchor" aria-hidden="true" href="#swin-transformer-hierarchical-vision-transformer-using-shifted-windows">#</a></h1>
<ul>
<li>论文地址: <a href="https://arxiv.org/abs/2103.14030">https://arxiv.org/abs/2103.14030</a></li>
<li>参考代码：<a href="https://github.com/microsoft/Swin-Transformer">https://github.com/microsoft/Swin-Transformer</a></li>
</ul>
<p>2021 ICCV best paper，提出了一个Transformer的通用backbone</p>
<p>远看是Transformer, 近看是CNN。</p>
<h2 id="transformer-in-cv的问题">Transformer in CV的问题<a hidden class="anchor" aria-hidden="true" href="#transformer-in-cv的问题">#</a></h2>
<p>ViT出来之后，虽然follow它的工作很多，但是很少有人能够提出一个通用的骨干网络来解决CV的各种任务。作者认为Transformer转换到CV领域的主要困难来自NLP和CV两大领域的形态不同，比如：</p>
<ol>
<li>NLP中的问题规模和CV不一样，NLP的scale通常是固定的，但是CV中的scale是变化的。</li>
<li>NLP里面的token就是一个单词，整个序列往往不会太大，但是CV的分辨率是非常大的，计算复杂度以二次方增加。</li>
</ol>
<p>Swin Transformer解决的方法：</p>
<ol>
<li>在局部计算self-attention。</li>
<li>采用层级结构。</li>
<li>采用shift-window打通不同的window之间的信息。(模型的名字就来自<strong>s</strong>hift-<strong>win</strong>dow)</li>
</ol>
<p><img loading="lazy" src="/src/swin_vit.png" alt=""  />
</p>
<p>左图是Swin Transformer的结构，把图像划分为patch，对在同一window之类的patch计算attention，然后通过合并patch降低分辨率，不断重复操作。右图是ViT的结构，把图像划分为patch，然后始终在所有patch之间做self-attention。</p>
<p>对比两个结构，Swin Transformer基本放弃使用了ViT的思想，转而借鉴了很多CNN的结构。比如说局部self-attention对比局部卷积域，shift-window对比滑动窗口，并且两者都采用了层叠结构。ViT讲求的是始终考虑全局所有信息计算attenntion，而Swin Transformer仅考虑窗口内的信息计算attention。两者都有降低计算复杂度的目的，但是一个希望保持注意力计算域在整张图片上，所以着力减少token数量，另一个选择了减小注意力计算域，然后通过层叠结构逐步扩展到整张图片上，所以可以把patch划得更小。</p>
<h2 id="swin主要结构">Swin主要结构<a hidden class="anchor" aria-hidden="true" href="#swin主要结构">#</a></h2>
<p><img loading="lazy" src="/src/swin_archtecture.png" alt=""  />
</p>
<p>参照上图，因为Transformer操作的基本单位都是patch，所以Swin Transformer也是首先进行patch partition，文中采用的是4×4的大小。这样一个patch就会包含4×4×3=48的维度，经过linear embedding layer把维度投影到C，分辨率变为$\frac{H}{4}\times\frac{W}{4}$。然后会经过两个Swin Transformer Block计算注意力。这个过程属于上图的stage1。</p>
<p>在stage2、stage3、stage4中，首先会把相邻的2×2个patch 融合到一起成为新patch，这样得到的patch维度会是4C，需要使用一个linear layer把维度投影到2C。之后经过数量不等的Swin Transformer Block计算注意力。经过这样的一个流程，特征图的分辨率会从$\frac{H}{4}\times\frac{W}{4}$依次下降到$\frac{H}{8}\times\frac{W}{8}$、$\frac{H}{16}\times\frac{W}{16}$、$\frac{H}{32}\times\frac{W}{32}$，也就是一个分层结构。</p>
<h2 id="swin-transformer-block结构">Swin Transformer Block结构<a hidden class="anchor" aria-hidden="true" href="#swin-transformer-block结构">#</a></h2>
<p>Swin Transformer Block是Swin Transformer的关键。标准的Transformer都是在全局进行注意力计算的，但是这样会导致计算复杂度二次上升。Swin Transformer采用了在window中进行计算的方式。</p>
<p>A×B大小的矩阵和B×C大小的矩阵相乘计算复杂度为ABC，在self-attention中，所有的线性变换都是以矩阵乘法的形式得到的。self-attention的公式如下：</p>
<p>$$\text{Attention}(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$</p>
<p>对于分辨率(patch数量)为$h \times w$的特征图，它的输入为$hw\times C$，首先需要经过三个$C\times C$的矩阵分别得到Q，K，V，那么复杂度一共就为$3hwC^2$。
计算$QK^T$的时候，是$hw\times C$的矩阵和$C\times hw$的矩阵相乘，计算复杂度为$(hw)^2C$。
计算$(QK^T)V$的时候，是$hw\times hw$的矩阵和$hw\times C$的矩阵相乘，计算复杂度为$(hw)^2C$。
在输出的时候还需要经过一次线性变换，乘上一个$C\times C$的矩阵，计算复杂度为$hwC^2$。</p>
<p>所以self-attention的复杂度一共为:$4hwC^2+2(hw)^2C$。</p>
<p>Window based Self-Attention前面得到QKV的过程和最后的线性变换和self-attention保持一致，但是计算$QK^T$变成了$\frac{h}{M}\times\frac{w}{M}$个$(M\times M)^2C$，计算$(QK^T)V$变成了$\frac{h}{M}\times\frac{w}{M}$个$(M\times M)^2C$。一共是$2M^2hwC$。</p>
<p>所以Window based Self-Attention复杂度一共为$4hwC^2+2M^2hwC$。</p>
<p><img loading="lazy" src="/src/W-MSA.png" alt=""  />
</p>
<p>由于划分window的方法虽然能够降低计算复杂度，但是相当于破坏了attention的全局性，计算局限于window中，为了打破不同window之间的壁垒，作者采用了shift-window的方法。具体来说，就是把window移动$(\lfloor\frac{M}{2}\rfloor, \lfloor\frac{M}{2}\rfloor)$之后重新划分。参见下图：</p>
<p><img loading="lazy" src="/src/shift-window.png" alt=""  />
</p>
<p>但是这样会带来问题，第一是window的个数由原来的4个变成了9个，第二是window中的patch数量不固定，所以作者采用了efficient batch computation的方法，如下图:</p>
<p><img loading="lazy" src="/src/Efficient_batch_computation.png" alt=""  />
</p>
<p>其实也就是通过类似circular padding的方式把不完整的几个window给补全。</p>
<p>但是在比如右下的window中，一个window包含了4个不同位置的patch，计算他们之间的attention是没有意义的，所以需要给它们添加mask，保证来自A的patch只跟来自A的patch算attention,来自B的patch只跟来自B的patch算attention。</p>
<p>另外，Swin Transformer里面还添加了relative position bias，具体添加位置参照文中的公式，就是那个B：</p>
<p>$$
\text{Attention}(Q,K,V)=\text{SoftMax}\left(QK^T/\sqrt{d}+B\right)V
$$</p>
<p>源码里面是构造一个可以学习的位置参数表，然后再建立一个相对位置索引表。对于两个patch，首先计算它们的相对位置索引，然后根据相对位置索引去找到对应的bias。</p>
<p>在一个Swin Transformer Block中，会首先进行一次不移动窗口的MSA计算以及依次移动窗口的MSA计算，这样整个过程表述为：</p>
<p>$$
\hat{z}^l=W-MSA(LN(z^{l-1}))+z^{l-1},\\
z^l = MLP(LN(\hat{z}^l))+\hat{z}^l,\\
\hat{z}^{l+1} = SW-MSA(LN(z^l))+z^l,\\
z^{l+1} = MLP(LN(\hat{z}^{l+1}))+\hat{z}^{l+1},
$$
<img loading="lazy" src="/src/swin_archtecture.png" alt=""  />
</p>
<h2 id="代码复现">代码复现<a hidden class="anchor" aria-hidden="true" href="#代码复现">#</a></h2>
<p>划分window以及逆过程代码:</p>
<pre><code>def partition(input, window_size):
    '''
    input: [B, C, H, W]
    windows_size: int
    把图片变成window
    return: [B, N, C, window_size, window_wize] N=H//window_size*W//window_size
    '''
    assert len(input.shape) == 4
    B, Channel, H, W = input.shape
    assert H % window_size == 0 and W % window_size == 0
    
    #不考虑B和C，由于reshape不会改变底层一维数组分布，所以需要把H改成[窗口数，窗口里面的H数]
    #W改成[窗口数，窗口里面的W数]才符合按行排布的一维数组情况,顺序不能反。
    output = input.reshape(B, Channel, H//window_size, window_size, W//window_size, window_size)
    #由于我们想要的输出最后两维应该是一个patch的二维形式，所以需要把维度变换过去
    output = output.permute(0, 2, 1, 4, 3, 5).reshape(B, Channel, -1, window_size, window_size).permute(0, 2, 1, 3, 4)
    return output

def reverse(input):
    '''
    input: [B, N, C, window_size, window_size]
    partition的逆变换
    return: [B, C, H, W]
    '''
    assert len(input.shape) == 5
    
    B, N, C, window_size, _ = input.shape
    x = input.permute(0, 2, 1, 3, 4)
    x = x.reshape(B, C, int(N**0.5), int(N**0.5), window_size, window_size)
    x = x.permute(0, 1, 2, 4, 3, 5)
    x = x.reshape(B, C, int(N**0.5)*window_size, int(N**0.5)*window_size)

    return x
</code></pre>
<p>patch embedding代码:</p>
<pre><code>class Patch_Linear_Embedding(nn.Module):
    def __inti__(self, patch_size, dim_in, dim_out, norm = None):
        '''
        根据patch_size划分并且投影到dim_out上，仍然是二维的
        '''
        super(Patch_Linear_Embedding, self).__init__()
        self.linear_embedding = nn.Conv2d(dim_in, dim_out, kernel_size=patch_size, stride=patch_size, padding=0)
        self.norm = norm
    
    def forward(self, input):
        '''
        input: [B, C, H, W]
        '''
        x = self.linear_embedding(x)#[B, C, H, W]-&gt;[B, dim_out, num_patch, num_patch]
        x = x.reshape(x.shape[0], x.shape[1], -1)
        x = x.permute(0, 2, 1)

        if self.norm is not None:
            x = self.norm(x)

        return x
</code></pre>
<p>MLP代码：</p>
<pre><code>class MLP(nn.Module):
    def __init__(self, dim_in, dim_out, dim_hidden, active_layer = nn.GELU, dropout = 0):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(dim_in, dim_hidden)
        self.active_layer = active_layer()
        self.fc2 = nn.Linear(dim_hidden, dim_out)
        self.Dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.fc1(x)
        x = self.active_layer(x)
        x = self.Dropout(x)#这里面使用了两个Dropout
        x = self.fc2(x)
        x = self.Dropout(x)
        return x
</code></pre>
<p>添加relative position bias：</p>
<pre><code>class Relative_Postion_Bias(nn.Module):
    '''
    输入: [B*N*num_head, L, C]
    '''
    def __init__(self, num_head, M):
        super(Relative_Postion_Bias, self).__init__()
        self.bias = nn.Parameter(torch.zeros((2*M-1, 2*M-1, num_head)))#构造一个bias表
        self.num_head = num_head
        #以下查询方式都是基于window中的patch是按照行优先顺序展开的
        coords = torch.stack(torch.meshgrid([torch.arange(M), torch.arange(M)]))
        coords = torch.flatten(coords, 1)#绝对坐标
        relative_coords = coords[:, :, None]-coords[:, None, :]#相对坐标
        relative_coords = relative_coords.permute(1, 2, 0)#[M**2, M**2, 2]
        relative_coords += M-1#索引置为非负数
        self.relative_coords = relative_coords

    def forward(self, input):
        '''
        input: [B*N*num_head, N, N]
        '''
        relative_position_bias = self.bias[self.relative_coords[:, :, 0], self.relative_coords[:, :, 1]]
        relative_position_bias = relative_position_bias.permute(2, 0, 1)
        relative_position_bias = relative_position_bias.unsqueeze(0).repeat(input.shape[0]//self.num_head)
        relative_position_bias = relative_position_bias.reshape(input.shape[0], input.shape[1], input.shape[2])
        
        return input + relative_position_bias
</code></pre>
<p>patch merging代码：</p>
<pre><code>class Patch_Merging(nn.Module):
    def __init__(self, dim_in, dim_out, stride):
        super(Patch_Merging, self).__init__()
        self.fc = nn.Linear(dim_in*stride**2, dim_out)
        self.stride = stride
    
    def forward(self, input):
        x = partition(input, self.stride)
        x.resahpe(x.shape[0], x.shape[1], -1)
        x = self.fc(x)
        x = x.reshape(x.shape[0], x.shape[1], -1, 1, 1)
        x = reverse(x)

        return x
</code></pre>
<p>W-MSA及SW-MSA：</p>
<pre><code>class Window_Multihead_Attention(nn.Module):
    def __init__(self, dim, num_head, window_size):
        self.dim = dim
        self.num_head = num_head
        #Swin默认这里的线性变换不改变维度，所以我也直接用了3*dim
        #Swin后面做multi-head的时候是直接把3*dim拆成num_head个来用，我认为在这里变成3*dim*num_head然后拆成num_head个效果是一样的
        self.qkv = nn.Linear(dim, 3*dim)

        self.relative_position_bias = Relative_Postion_Bias(num_head, window_size)


    def forward(self, input, mask = None):
        '''
        input: [B*N, L, C], L是把window中的patch按行优先展开的数量
        mask: [B*N, L, L], L同上
        '''

        #[B*N, L, C]-&gt;[B*N, L, 3, num_head, C//num_head]
        qkv = self.qkv(input).\
            reshape(input.shape[0], input.shape[1], 3, self.num_head, input.shape[2]//self.num_head)
        qkv = qkv.permute(0, 3, 2, 1, 4).reshape(input.shape[0]*self.num_head, input.shape[1], 3, -1)
        
        Q = qkv[:, :, 0, :]#[B*N*num_head, L, C//num_head]
        K = qkv[:, :, 1, :]
        V = qkv[:, :, 2, :]

        attention = torch.bmm(Q, K.permute(0, 2, 1))/(self.dim//self.num_head)**0.5#计算相似度
        attention = self.relative_position_bias(attention)#加上Bias
        
        if mask is not None:
            weights = torch.exp(attention)
            weights = weights*(mask.unsqueeze(1).repeat(1, self.num_head, 1, 1)).reshape(-1, mask.shape[1], mask.shape[2])
            weights = weights/torch.sum(weights, -1)
        else:
            weights  =torch.softmax(weights, -1)
        
        output = torch.bmm(weights, V)

        return output+input
</code></pre>
<p>SwinTransformerBlock：</p>
<pre><code>class SwinTransformerBlock(nn.Moduel):
    def __init__(self, dim, num_head, window_size, mlp_dim_hidden):
        super(SwinTransformerBlock, self).__init__()

        self.LN1 = nn.LayerNorm(dim)
        self.LN2 = nn.LayerNorm(dim)
        self.LN3 = nn.LayerNorm(dim)
        self.LN4 = nn.LayerNorm(dim)

        self.WMSA = Window_Multihead_Attention(dim, num_head, window_size)
        self.SWMSA = Window_Multihead_Attention(dim, num_head, window_size)

        self.MLP1 = MLP(dim, dim, mlp_dim_hidden)
        self.MLP2 = MLP(dim, dim, mlp_dim_hidden)

        self.window_size = window_size

    def forward(self, input):
        '''
        input: [B, C, H, W]
        '''
        B, C, H, W = input.shape
        L = self.window_size**2
        N = H*W/L

        x = partition(x, self.window_size)
        x = x.reshape(B*N, C, L).permute(0, 2, 1)#[B*N, L, C]
        x = self.LN1(input)
        x = self.WMSA(x)
        x = x+input
        x = self.LN2(x)
        x = self.MLP1(x)
        x = x+input#[B*N, L, C]

        x = x.reshape(B, N, L, C).permute(0, 1, 3, 2).reshape(B, N, C, self.window_size, -1)
        x = reverse(x)
        shift = self.window_size//2
        x = torch.cat([torch.cat([x[:, :, shift:, shift:], x[:, :, shift:, :shift]], dim = -1), 
        torch.cat([x[:, :, :shift, shift:], x[:, :, :shift, :shift]], dim = -1)], dim = -2)

        mask = torch.ones(B, 1, H, W)
        mask[:, :, :, -shift:] = 2
        mask[:, :, -shift:, :] = 3
        mask[:, :, -shift:, -shift:] = 4
        origin = mask#[B, 1, H, W]
        mask = partition(mask, self.window_size)
        mask = mask.reshape(B, mask.shape[1], -1)#[B, N, L]
        mask = mask.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, self.window_size, self.window_size)#[B, N, L, ws, ws]
        mask = reverse(mask)#[B, L, H, W]
        mask = torch.where(mask==origin.repeat(1, L, 1, 1), 1, 0)

        x = self.LN3(input)
        x = self.SWMSA(x, mask)
        x = x+input
        x = self.LN4(x)
        x = self.MLP2(x)
        x = x+input#[B*N, L, C]
        
        return x
</code></pre>
<h2 id="实验结论">实验结论<a hidden class="anchor" aria-hidden="true" href="#实验结论">#</a></h2>
<p><img loading="lazy" src="/src/swin_classification.png" alt=""  />

在分类任务上，Swin表现大幅超过其他模型。在Image-21k预训练的结果也是更好。</p>
<p><img loading="lazy" src="/src/swin_object_detection.png" alt=""  />

在目标检测任务上，效果也是非常好。</p>
<p><img loading="lazy" src="/src/swin_segmentation.png" alt=""  />

swin实现了在这种dense prediction任务上采用Transformer的backbone，效果也是大幅超越其他模型。</p>
<p><img loading="lazy" src="/src/swin_pos.png" alt=""  />

关于shift-window以及relative position bias的消融实验。有了这两个东西都可以涨点。</p>
<p>值得一提的是，作者后面把Swin的架构用到MLP-Mixer上面，发现了效果也很好，说明Swin的结构是具有通用性的。但是这是不是也说明了Swin的结构中，Transformer的地位其实没有那么高呢，我个人感觉Swin其实融合了很多文章的trick，比如滑动窗口也是一些CNN在用的提速方法，所以Swin刚出来的时候其实有很多质疑的声音。但是不得不说Swin作为通用backbone的地位非常高，我拿这个backbone也在某个workshop上干到过第二（虽然后面掉了），Transformer的潜力还是很大的。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://venite-xjc.github.io/">My Blog Site</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
